---
title: "NifH_pipeline"
output: html_document
date: "2023-04-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{seq visualization w QIIME2}
#Import demultiplexed sequences into .qza artifact for QIIME2-related scripts:
cd /mnt/home/kristybr/20230410_Amplicon_KRI13538_PE250/

qiime tools import 
--type 'SampleData[PairedEndSequencesWithQuality]' \
--input-path 20230410_Amplicon_KRI13538_PE250_manifest.txt/ \
--input-format PairedEndFastqManifestPhred33V2 \
--output-path 20230410_Amplicon_KRI13538_PE250.qza \

# Visualize quality of raw, demultiplexed sequences. The .qzv file is available in this repository. 
qiime demux summarize \
--i-data 20230410_Amplicon_KRI13538_PE250.qza \
--o-visualization 20230410_Amplicon_KRI13538_PE250_SUMMARY_VIZ.qzv
```


```{merge sequences via USEARCH}
# Merge paired-end fastq sequences using usearch
for i in *R1*.fastq; do
  /mnt/research/rdp/public/thirdParty/usearch11.0.667_i86linux64  \
  -fastq_mergepairs $i \
  -fastqout merged_reads_${i}\
  -sample $i
done

# Concatenate all merged reads into one file
cat *merged_reads* > /merged/merged_reads.fastq

# Remove intermediate merge files
cd /mnt/home/kristybr/20230410_Amplicon_KRI13538_PE250/
rm *merged_reads*
```

```{seq_filter via USEAARCH}
# Filter merged fastq sequences: Sequences were quality and length filtered to maximum expected errors of 1 and minimum length of 380bp 
cd /mnt/home/kristybr/20230410_Amplicon_KRI13538_PE250/merged

/mnt/research/rdp/public/thirdParty/usearch11.0.667_i86linux64  \
-fastq_filter merged_reads.fastq \
-fastq_trunclen 380 
-fastq_maxee 1.0 \
-fastaout merged_reads_filtered.fa
```

```{Homolog removal}
# Screen merged, filtered reads using HMMER
hmmsearch --domtblout hmmOut1.out /mnt/home/kristybr/NifMAP/Resources/hmm_nuc_1160_nifH.hmm ./merged/merged_reads_filtered.fa

# Identify acceptable and shit hits 
awk '{print $1}' hmmOut1.out | grep -v "#" > acceptable_hits
grep ">" ./merged/merged_reads_filtered.fa | grep -v -F -f acceptable_hits > shit_hits
totalUnique = `grep ">" ./merged/merged_reads_filtered.fa | wc - l`
totalAccepted = `cat acceptable_hits | wc -l`
totalRemoved = `cat shit_hits | wc -l`

# Filter out unacceptable reads from merged_reads_filtered.fa
awk 'BEGIN{FS="\n";RS=">"};NR>1{print(">"$1);for(i=2;i<=NF;i++){printf($i)};print("")}' ./merged/merged_reads_filtered.fa | grep -A 1 -F -f acceptable_hits | grep -v "^\-\-$" > ./merged/merged_reads_filtered_hmm.fa

```

```{OTU generation}
# First, dereplicate the sequences
cd /mnt/home/kristybr/20230410_Amplicon_KRI13538_PE250

/mnt/research/rdp/public/thirdParty/usearch11.0.667_i86linux64 -fastx_uniques ./merged/merged_reads_filtered_hmm.fa \
-minuniquesize 2 \
-fastaout ./merged/unique.fa 

# Second, sort unique sequences by length
/mnt/research/rdp/public/thirdParty/usearch11.0.667_i86linux64 -sortbylength ./merged/unique.fa \
-fastout ./merged/unique_sorted.fa

# Third, cluster sequences into OTUs at 97% similarity
/mnt/research/rdp/public/thirdParty/usearch11.0.667_i86linux64 -cluster_fast ./merged/unique_sorted.fa \
-id 0.97 \
-centroids otus.fa \
-uc clusters.uc
```

```{OTU representative seq translation with Framebot}
cd /mnt/home/kristybr/20230410_Amplicon_KRI13538_PE250
eVal_chL=1e-50
score_chL=150
eVal_bchX=1e-50
score_bchX=150

# Translate OTU representative sequences into protein sequences
/mnt/home/kristybr/anaconda3/pkgs/rdptools-2.0.2-1/bin/FrameBot framebot \
-N -l 30 -i 0.4 \
-o ./nifH/ /mnt/home/kristybr/Framebot/refset/nifh_prot_ref.fasta otus.fa 
```

```{Filter out homologous genes (bchL; chlL; bchX; parA) using HMM:}
# Corrected AA sequences are in _corr_prot.fasta
# Screen with hmm to identify all hits
cd /mnt/home/kristybr/20230410_Amplicon_KRI13538_PE250

# First - press the homolog hmm prior to running hmmscan - code will not run unless you do this
cd /mnt/home/kristybr/NifMAP/Resources/
hmmpress nifH_chlL_bchX.hmm

# Now run hmmscan
hmmscan --cpu 32  --domtblout hmmOut2.out /mnt/home/kristybr/NifMAP/Resources/nifH_chlL_bchX.hmm ./nifH/_corr_prot.fasta

# Filter out homologous sequences
cat hmmOut2.out | awk 'NR>3{if($8>bitarray[$4]){bitarray[$4]=$8;outArray[$4]=$1"\t"$4}}END{for(entry in outArray){print outArray[entry]}}' > assignments.txt



grep "nifH" assignments.txt | awk '{print $2}' | sort > acceptable_hits
grep ">" ./nifH/_corr_nucl.fasta | awk '{print $1}' | grep -v -F -f acceptable_hits | sed 's/>//'> shitHits
totalOTUs=grep ">" ./nifH/_corr_nucl.fasta | wc -l
totalAccepted=cat acceptable_hits | wc -l
totalRemoved=cat shitHits | wc -l


#mv ${RESULTSFOLDER}/${inputReads} ${WORKFOLDER}/${inputReads}
cat ./nifH/_corr_nucl.fasta | sed 's/ //g' | awk 'BEGIN{RS=">";FS="\n"};NR>1{printf $1"_\t";for(i=2;i<=NF;i++){printf($i)}print("")}' | grep -F -f acceptable_hits | awk '{gsub("_$","",$1);print(">"$1"\n"$2)}' > nifH_corr_nucl_only_nifH.fasta
cat ./nifH/_corr_prot.fasta | sed 's/ //g' |  awk 'BEGIN{RS=">";FS="\n"};NR>1{printf $1"_\t";for(i=2;i<=NF;i++){printf($i)}print("")}' | grep -F -f acceptable_hits | awk '{gsub("_$","",$1);print(">"$1"\n"$2)}' > nifH_corr_prot.fasta
cat ${WORKFOLDER}/nifH_corr_prot.fasta | sed 's/ //g' |  awk 'BEGIN{RS=">";FS="\n"};NR>1{printf $1"_\t";for(i=2;i<=NF;i++){printf($i)}print("")}' | grep -F -f shitHits | awk '{gsub("_$","",$1);print(">"$1"\n"$2)}' > nifH_rej_prot.fasta
```

```{Construct OTU table based on filtered, corrected sequences}
cd /mnt/home/kristybr/20230410_Amplicon_KRI13538_PE250

# Make an OTU table based on filter corrected seqs
/mnt/research/rdp/public/thirdParty/usearch11.0.667_i86linux64 -otutab ./merged/merged_reads_filtered_hmm.fa \
-otus nifH_corr_nucl_only_nifH.fasta \
-otutabout otu_table.txt \
-mapout map.txt \
-threads 32
```

```{Assign taxonomy to OTUs}
# Run kraken2 on representative OTUs
kraken2 --db /mnt/research/EvansLab/DATABASES/kraken2_db_feb23/ \
nifH_OTU.fasta \
--threads 32 \
--output nifH_taxonomy.output \
--report nifH_taxonomy_report.txt

# Reformat Kraken2's output with NCBI taxononmy -- use the nt database. The database MUST have the associated taxDB files. 
# This will add all taxonomic information based on the NCBI taxid
python3 /mnt/home/kristybr/CGATMetaSequencing/scripts/translateKraken2.py \
--krakenout nifH_taxonomy.output \
--translatedout nifH_taxonomy_reformatted.out \
--taxdatadir /mnt/research/EvansLab/DATABASES/NCBInt_feb23/
```